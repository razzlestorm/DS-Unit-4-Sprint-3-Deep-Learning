{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http.client.HTTPResponse object at 0x0000020EEDCF4748>\n",
      "﻿\n",
      "Project Gutenberg’s The Complete Works of William Shakespeare, by William\n",
      "Shakespeare\n",
      "\n",
      "This eB\n"
     ]
    }
   ],
   "source": [
    "data = urlopen('https://www.gutenberg.org/files/100/100-0.txt')\n",
    "\n",
    "print(data)\n",
    "\n",
    "def get_data(x):\n",
    "    string = []\n",
    "    for line in x:\n",
    "        string.append(line.decode('utf-8'))\n",
    "        \n",
    "    return string\n",
    "\n",
    "shakes = get_data(\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(shakes[2900:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(shakes))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 1148003\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in shakes]\n",
    "\n",
    "sequences = [] # Each element is 40 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i, t, char] = 1\n",
    "    \n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\csfiles\\lambda\\git\\deep_learning\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)))) #input_shape of singular observation\n",
    "model.add(Dense(len(chars), activation='softmax')) #predicting probability of which character is next\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(shakes[2900:]) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = shakes[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_int[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = int_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1148003 samples\n",
      "Epoch 1/3\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.6330\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" naked soul,\n",
      "Which I’ll entreat to lead\"\n",
      " naked soul,\n",
      "Which I’ll entreat to lead and the comes the fair him\n",
      "                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" naked soul,\n",
      "Which I’ll entreat to lead\"\n",
      " naked soul,\n",
      "Which I’ll entreat to lead him come, and here.\n",
      "\n",
      "ORINGO.\n",
      "I'll all than the graning thee is new then.\n",
      "    Here with the down then I make some hasself,\n",
      "These come the cidection upon of the dount.\n",
      "\n",
      "AMBALLA.\n",
      "In the lies, and the hearter to the sane\n",
      "    The come from the prease for the King one sweet to the ears\n",
      "To strands the must be of it thee.\n",
      "                                                                        \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" naked soul,\n",
      "Which I’ll entreat to lead\"\n",
      " naked soul,\n",
      "Which I’ll entreat to lead, and aswout, my lash.—\n",
      "My Lickin and the Barncily.\n",
      "  MORIT'S Enter the juster, coulds, Were dispurater.\n",
      "\n",
      "Now that I am any hoands, all still, a\n",
      "    A cannot bead proon tear yet this forth fere in\n",
      "    And eyds on ’treattard, supengs and madate:\n",
      "    Hath restept twas leash ip mecher's.\n",
      "\n",
      "KENT.\n",
      "Or our is, on, his grease?\n",
      "  ANME. How doth mels their forsain host they aid\n",
      "With doy’s friends\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" naked soul,\n",
      "Which I’ll entreat to lead\"\n",
      " naked soul,\n",
      "Which I’ll entreat to leaded shaa, arvisour.\n",
      "\n",
      "AMOLATRE.\n",
      "They, shall that Actalloo\n",
      "Three inan;—\n",
      "\n",
      "HUBONIABOY.\n",
      "Thinkes, Call, herfel-touto to rengs)ieve\n",
      "I mulken shors Henbyen leaks, good every\n",
      "Brayself mere-so heart; Byanct tonexil have at evely to. Kinn?\n",
      "\n",
      "SLEOPE INoren.\n",
      "Fles_Muntune, on fort._ thou seed ences mertaims me, bade,\n",
      "(Wilk they days in’t kincow dides forlive.\n",
      "Dray. But shalite mamion, I am a frock m\n",
      "1148003/1148003 [==============================] - 764s 666us/sample - loss: 1.6330\n",
      "Epoch 2/3\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.5883\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"id deceit, I mean to learn;\n",
      "    For it \"\n",
      "id deceit, I mean to learn;\n",
      "    For it with the strenger the should the stands to the sen shall be my to his heart\n",
      "    The stranger and the senter the forth and so strenger the strenged to her to him.\n",
      "\n",
      " [_Exeunt._]\n",
      "\n",
      "SCENE II. The sen offer and such a sand of the father sonest to me.\n",
      "    The streath and the forth and the stands and the strange.\n",
      "    I am the strange of the send and the strange.\n",
      "    The sen the stand the father th\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"id deceit, I mean to learn;\n",
      "    For it \"\n",
      "id deceit, I mean to learn;\n",
      "    For it the shall as the baniss the strangeth entlemen\n",
      "    The rest the reast and the fair of the sen do\n",
      "                                                                        [They she cansel offellons.\n",
      "  LEOR. Why is no make these you be not worth me.\n",
      "\n",
      "LOWN.\n",
      "Where there there it not be.\n",
      "\n",
      " Enter Gloudan._]\n",
      "\n",
      "PARDARD.\n",
      "I would to the fortune of the see dispotester and fill shall sen sweet such a\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"id deceit, I mean to learn;\n",
      "    For it \"\n",
      "id deceit, I mean to learn;\n",
      "    For it duty threar by wordiful judgling shrands,\n",
      "He with they put on, bellous'd him not see;\n",
      "    This with boar ealsesh frief and whick\n",
      "    And wheles at your fould past care him best: which,\n",
      "Strom or a bed.\n",
      "But surd perfore be strord bid off; but 'am-fill we hand.\n",
      "    Buse get they san lite this villain lick.\n",
      "    And traw you Here th' gelles with your of.\n",
      "\n",
      "SEONTOBRAN.\n",
      "Alatios.\n",
      "\n",
      "SEYNCILE.\n",
      "Fa\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"id deceit, I mean to learn;\n",
      "    For it \"\n",
      "id deceit, I mean to learn;\n",
      "    For it is us! To like own puck rilet\n",
      "And to fropery: meand when his liggrous to thy\n",
      "the idlester;\n",
      "    To Gloven, I know your more and writife,\n",
      "      he thoughts of bulesded on eltsperss.  hel\n",
      "To pun did of traete forting.\n",
      "\n",
      "FISS LUQUE.\n",
      "O was rename; atmsharve, iul a men.\n",
      "\n",
      "AMORANL.\n",
      "Upince thee, From no fast welcrot me to!\n",
      "Good of purlolcup I sting Jowfy; these,\n",
      "To I loagt not rame they shapt f\n",
      "1148003/1148003 [==============================] - 772s 673us/sample - loss: 1.5883\n",
      "Epoch 3/3\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.5536\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" he suited to his watery tomb.\n",
      "If spiri\"\n",
      " he suited to his watery tomb.\n",
      "If spirit the word for the man the sound the word,\n",
      "    The see the see the worthing the rease of my sears,\n",
      "    And the word the see the see the see thee,\n",
      "    And the streath and the seed to the see\n",
      "    That the see the see the word to the see the word to me the see\n",
      "                                                                                                                                         \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" he suited to his watery tomb.\n",
      "If spiri\"\n",
      " he suited to his watery tomb.\n",
      "If spirit too, the countard me that well the beht\n",
      "    That be done divery to the son the selve me were honours more,\n",
      "    And he hang him all but the thousand his lively.\n",
      "\n",
      "SECOND.\n",
      "You are been the ruble, and were see your here.\n",
      "    There away a much and one doth his took and do not bed when thee,\n",
      "    The sheak not for mear of her some of the forth,\n",
      "    And the word and be the word to the man forth \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" he suited to his watery tomb.\n",
      "If spiri\"\n",
      " he suited to his watery tomb.\n",
      "If spiriteds to out anister to my faletherefidess,\n",
      "Will cart on thy serveh to hath your Learation, Toblecken in here’s to\n",
      "a the Quelsinates, my Armman, though a\n",
      "      Enter Cwelucas. BERDY MPROUCHS Withant]  put me not shooke have must eve\n",
      "\n",
      "    Oul would I spercy, come husblate illadons his blood.\n",
      "\n",
      "Loch she here, mort to my lovity? Withous on\n",
      "These dreistime him and was notorrove me, eue some foug\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" he suited to his watery tomb.\n",
      "If spiri\"\n",
      " he suited to his watery tomb.\n",
      "If spirithing dry warks me appacian:\n",
      " king touth, but you our quacks' of\n",
      "wishibll; beoods, I will by Brutter in.\n",
      "Kitg again.\n",
      "At you fought keeno sadar?\n",
      "\n",
      "DOSTAMO.\n",
      "[_Joun.\n",
      "\n",
      "LUCIO.\n",
      "Gear, my guenom? Mastit a ambindes!\n",
      "More!\n",
      "\n",
      "[To QUEEN. He moyly in my.\n",
      "  QUEEN. Methough hap I wordiverding qoot me, helf thunk\n",
      "Dogget this power as do; whyonded emoull.\n",
      "\n",
      "ELIONIANA.\n",
      "And all thee peese!t, son your \n",
      "1148003/1148003 [==============================] - 722s 629us/sample - loss: 1.5536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e9975eda0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=3,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
